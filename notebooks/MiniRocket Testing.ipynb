{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "congressional-classification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joseph/Coding/ml_projects/g2net_ml_dl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../python\")\n",
    "import g2net\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "prime-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_core import default_device, apply_init\n",
    "from fastai.layers import LinBnDrop, flatten_model\n",
    "from fastai.basics import Learner\n",
    "from g2net.utils.tsai import print_verbose\n",
    "from functools import partial\n",
    "from torch.nn import Flatten, Sequential, Linear, Module\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "def is_linear(l):\n",
    "    return isinstance(l, Linear)\n",
    "\n",
    "\n",
    "def ifnone(a, b):\n",
    "    # From fastai.fastcore\n",
    "    \"`b` if `a` is None else `a`\"\n",
    "    return b if a is None else a\n",
    "\n",
    "\n",
    "class Reshape(Module):\n",
    "\n",
    "    def __init__(self, *shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.shape[0], *self.shape)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({', '.join(['bs'] + [str(s) for s in self.shape])})\"\n",
    "\n",
    "\n",
    "class create_lin_3d_head(Sequential):\n",
    "    \"Module to create a 3d output head with linear layers\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_in,\n",
    "                 n_out,\n",
    "                 seq_len,\n",
    "                 d=(),\n",
    "                 lin_first=False,\n",
    "                 bn=True,\n",
    "                 act=None,\n",
    "                 fc_dropout=0.):\n",
    "\n",
    "        assert len(\n",
    "            d) == 2, \"you must pass a tuple of len == 2 to create a 3d output\"\n",
    "        layers = [Flatten()]\n",
    "        layers += LinBnDrop(n_in * seq_len,\n",
    "                            n_out,\n",
    "                            bn=bn,\n",
    "                            p=fc_dropout,\n",
    "                            act=act,\n",
    "                            lin_first=lin_first)\n",
    "        layers += [Reshape(*d)]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "def noop(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_layers(model, cond=noop, full=True):\n",
    "    if isinstance(model, Learner):\n",
    "        model = model.model\n",
    "    if full:\n",
    "        return [m for m in flatten_model(model) if any([c(m) for c in L(cond)])]\n",
    "    else:\n",
    "        return [m for m in model if any([c(m) for c in L(cond)])]\n",
    "\n",
    "\n",
    "def get_nf(m):\n",
    "    \"Get nf from model's first linear layer in head\"\n",
    "    return get_layers(m[-1], is_linear)[0].in_features\n",
    "\n",
    "\n",
    "def transfer_weights(model,\n",
    "                     weights_path: Path,\n",
    "                     device: torch.device = None,\n",
    "                     exclude_head: bool = True):\n",
    "    \"\"\"Utility function that allows to easily transfer weights between models.\n",
    "    Taken from the great self-supervised repository created by Kerem Turgutlu.\n",
    "    https://github.com/KeremTurgutlu/self_supervised/blob/d87ebd9b4961c7da0efd6073c42782bbc61aaa2e/self_supervised/utils.py\"\"\"\n",
    "\n",
    "    device = ifnone(device, default_device())\n",
    "    state_dict = model.state_dict()\n",
    "    new_state_dict = torch.load(weights_path, map_location=device)\n",
    "    matched_layers = 0\n",
    "    unmatched_layers = []\n",
    "    for name, param in state_dict.items():\n",
    "        if exclude_head and 'head' in name:\n",
    "            continue\n",
    "        if name in new_state_dict:\n",
    "            matched_layers += 1\n",
    "            input_param = new_state_dict[name]\n",
    "            if input_param.shape == param.shape:\n",
    "                param.copy_(input_param)\n",
    "            else:\n",
    "                unmatched_layers.append(name)\n",
    "        else:\n",
    "            unmatched_layers.append(name)\n",
    "            pass  # these are weights that weren't in the original model, such as a new head\n",
    "    if matched_layers == 0:\n",
    "        raise Exception(\"No shared weight names were found between the models\")\n",
    "    else:\n",
    "        if len(unmatched_layers) > 0:\n",
    "            print(f'check unmatched_layers: {unmatched_layers}')\n",
    "        else:\n",
    "            print(f\"weights from {weights_path} successfully transferred!\\n\")\n",
    "\n",
    "\n",
    "def build_ts_model(arch,\n",
    "                   c_in=None,\n",
    "                   c_out=None,\n",
    "                   seq_len=None,\n",
    "                   d=None,\n",
    "                   dls=None,\n",
    "                   device=None,\n",
    "                   verbose=False,\n",
    "                   pretrained=False,\n",
    "                   weights_path=None,\n",
    "                   exclude_head=True,\n",
    "                   cut=-1,\n",
    "                   init=None,\n",
    "                   arch_config={},\n",
    "                   **kwargs):\n",
    "\n",
    "    device = ifnone(device, default_device())\n",
    "    if dls is not None:\n",
    "        c_in = ifnone(c_in, dls.vars)\n",
    "        c_out = ifnone(c_out, dls.c)\n",
    "        seq_len = ifnone(seq_len, dls.len)\n",
    "\n",
    "    if sum([\n",
    "            1 for v in [\n",
    "                'RNN_FCN', 'LSTM_FCN', 'RNNPlus', 'LSTMPlus', 'GRUPlus',\n",
    "                'InceptionTime', 'TSiT', 'GRU_FCN', 'OmniScaleCNN', 'mWDN',\n",
    "                'TST', 'XCM', 'MLP', 'MiniRocket', 'InceptionRocket'\n",
    "            ] if v in arch.__name__\n",
    "    ]):\n",
    "        print_verbose(\n",
    "            f'arch: {arch.__name__}(c_in={c_in} c_out={c_out} seq_len={seq_len} device={device}, arch_config={arch_config}, kwargs={kwargs})',\n",
    "            verbose)\n",
    "        model = arch(c_in, c_out, seq_len=seq_len, **arch_config,\n",
    "                     **kwargs).to(device=device)\n",
    "    elif 'minirockethead' in arch.__name__.lower():\n",
    "        print_verbose(\n",
    "            f'arch: {arch.__name__}(c_in={c_in} seq_len={seq_len} device={device}, arch_config={arch_config}, kwargs={kwargs})',\n",
    "            verbose)\n",
    "        model = (arch(c_in, c_out, seq_len=1, **arch_config,\n",
    "                      **kwargs)).to(device=device)\n",
    "    elif 'rocket' in arch.__name__.lower():\n",
    "        print_verbose(\n",
    "            f'arch: {arch.__name__}(c_in={c_in} seq_len={seq_len} device={device}, arch_config={arch_config}, kwargs={kwargs})',\n",
    "            verbose)\n",
    "        model = (arch(c_in=c_in, seq_len=seq_len, **arch_config,\n",
    "                      **kwargs)).to(device=device)\n",
    "    else:\n",
    "        print_verbose(\n",
    "            f'arch: {arch.__name__}(c_in={c_in} c_out={c_out} device={device}, arch_config={arch_config}, kwargs={kwargs})',\n",
    "            verbose)\n",
    "        model = arch(c_in, c_out, **arch_config, **kwargs).to(device=device)\n",
    "\n",
    "    try:\n",
    "        model[0], model[1]\n",
    "        subscriptable = True\n",
    "    except:\n",
    "        subscriptable = False\n",
    "    if hasattr(model, \"head_nf\"):\n",
    "        head_nf = model.head_nf\n",
    "    else:\n",
    "        try:\n",
    "            head_nf = get_nf(model)\n",
    "        except:\n",
    "            head_nf = None\n",
    "\n",
    "    if not subscriptable and 'Plus' in arch.__name__:\n",
    "        model = Sequential(*model.children())\n",
    "        model.backbone = model[:cut]\n",
    "        model.head = model[cut:]\n",
    "\n",
    "    if pretrained and not ('xresnet' in arch.__name__ and\n",
    "                           not '1d' in arch.__name__):\n",
    "        assert weights_path is not None, \"you need to pass a valid weights_path to use a pre-trained model\"\n",
    "        transfer_weights(model,\n",
    "                         weights_path,\n",
    "                         exclude_head=exclude_head,\n",
    "                         device=device)\n",
    "\n",
    "    if init is not None:\n",
    "        apply_init(model[1] if pretrained else model, init)\n",
    "\n",
    "    setattr(model, \"head_nf\", head_nf)\n",
    "    setattr(model, \"__name__\", arch.__name__)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "consecutive-equity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000e74ad</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/joseph/datasets/g2net_train/train/0/0/0/00000e74ad.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f4945</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/joseph/datasets/g2net_train/train/0/0/0/00001f4945.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000661522</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/joseph/datasets/g2net_train/train/0/0/0/0000661522.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00007a006a</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/joseph/datasets/g2net_train/train/0/0/0/00007a006a.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000a38978</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/joseph/datasets/g2net_train/train/0/0/0/0000a38978.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  target  \\\n",
       "0  00000e74ad       1   \n",
       "1  00001f4945       0   \n",
       "2  0000661522       0   \n",
       "3  00007a006a       0   \n",
       "4  0000a38978       1   \n",
       "\n",
       "                                                           path  \n",
       "0  /home/joseph/datasets/g2net_train/train/0/0/0/00000e74ad.npy  \n",
       "1  /home/joseph/datasets/g2net_train/train/0/0/0/00001f4945.npy  \n",
       "2  /home/joseph/datasets/g2net_train/train/0/0/0/0000661522.npy  \n",
       "3  /home/joseph/datasets/g2net_train/train/0/0/0/00007a006a.npy  \n",
       "4  /home/joseph/datasets/g2net_train/train/0/0/0/0000a38978.npy  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from g2net.io.subset import create_subset_df\n",
    "from pathlib import Path\n",
    "train_subset = create_subset_df(Path(\"/home/joseph/datasets/g2net_train/train\"), \"~/datasets/g2net_train/training_labels.csv\")\n",
    "print(len(train_subset))\n",
    "train_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "compact-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "from g2net.io.kfold import split_into_train_val_test\n",
    "from g2net.train import create_base_transforms, create_dataloaders\n",
    "\n",
    "train_idx, valid_idx, test_idx = split_into_train_val_test(train_subset, seed=420)\n",
    "train_fold = train_subset.iloc[train_idx]\n",
    "valid_fold=  train_subset.iloc[valid_idx]\n",
    "transforms = create_base_transforms()\n",
    "train_loader, valid_loader = create_dataloaders(train_fold, valid_fold, batch_size=64,\n",
    "                                                train_transforms=transforms[\"train\"],\n",
    "                                                test_transforms=transforms[\"test\"])\n",
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "falling-attempt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 4096])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = next(train_iter)\n",
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "comic-graham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "worldwide-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2net.models.filter import MiniRocketFeatures, MiniRocketHead, \\\n",
    "                                MiniRocket, get_minirocket_features\n",
    "from fastai.basics import DataLoaders\n",
    "\n",
    "dls = DataLoaders(train_loader, valid_loader)\n",
    "dls.vars = 3 # channels in, 3 signals\n",
    "dls.c = 1 # channels out (1 binary classification)\n",
    "dls.len = 4096 # length of a single signal\n",
    "model = build_ts_model(MiniRocket, dls=dls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "destroyed-franchise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniRocket(\n",
       "  (backbone): MiniRocketFeatures()\n",
       "  (head): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): BatchNorm1d(9996, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=9996, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-excerpt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
